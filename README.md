# Proyecto de Flujo de Datos de Bienes RaÃ­ces
## Fuente de los Datos
Los datos utilizados en este proyecto provienen de [Kaggle](https://www.kaggle.com/datasets/0e23f01c0fc5a3d7a83e20023c70534df3cbbc6c23f1baf19f2ae3961a1576d7).

![ARQ](sources/arq.png)

## Objetivo
El objetivo de este proyecto es procesar y analizar datos de bienes raÃ­ces para generar un conjunto de datos final (`final_real_estate_insumo`) con columnas adicionales calculadas. Este conjunto de datos se utilizarÃ¡ para anÃ¡lisis y obtenciÃ³n de informaciÃ³n.

## Preguntas a Abordar
- Â¿Edad de la casa en aÃ±os?
- Â¿CÃºal es el precio por pie cuadrado?
- Â¿CÃºal es el nÃºmero total de habitaciones?
- Â¿CÃºal es el precio por habitaciÃ³n?
- Â¿CÃºal es el precio de la propiedad por tamaÃ±o del terreno?

## ElecciÃ³n del Modelo
El modelo elegido para este proyecto implica el uso de AWS Glue para operaciones ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) y Apache Spark para el procesamiento de datos. Esta elecciÃ³n se hizo debido a la escalabilidad y flexibilidad de estas herramientas para manejar grandes conjuntos de datos y transformaciones complejas.

## Enfoque para Diferentes Escenarios

### Si los Datos se Incrementan en 100x
- **Escalabilidad**: Aprovechar la escalabilidad de AWS Glue y Spark aumentando el nÃºmero de nodos de trabajo y utilizando particionamiento para manejar conjuntos de datos mÃ¡s grandes de manera eficiente.
- **Almacenamiento**: Utilizar Amazon S3 para almacenamiento escalable y considerar el uso de formatos de almacenamiento columnar como Parquet para consultas eficientes.

### Si las TuberÃ­as se Ejecutan Diariamente en una Ventana de Tiempo EspecÃ­fica
- **ProgramaciÃ³n**: Utilizar AWS Glue Workflows y AWS Step Functions para programar y gestionar los trabajos ETL para que se ejecuten dentro de la ventana de tiempo especificada.
- **Procesamiento Incremental**: Implementar procesamiento de datos incremental para manejar actualizaciones diarias de datos de manera eficiente.

### Si la Base de Datos Necesita Ser Accedida por MÃ¡s de 100 Usuarios Funcionales
- **Concurrencia**: Utilizar Amazon Redshift o Amazon Aurora para manejar alta concurrencia y proporcionar un rendimiento rÃ¡pido en las consultas.
- **CachÃ©**: Implementar mecanismos de cachÃ© utilizando Amazon ElastiCache para reducir la carga en la base de datos.

### Si se Requiere Hacer AnalÃ­tica en Tiempo Real
- **Procesamiento en Tiempo Real**: Utilizar AWS Kinesis o Apache Kafka para la ingesta y procesamiento de datos en tiempo real.
- **AnalÃ­tica de Streaming**: Implementar analÃ­tica de streaming utilizando Apache Spark Streaming o AWS Kinesis Data Analytics.

## ParÃ¡metros del Workflow
Las siguientes son las propiedades de ejecuciÃ³n predeterminadas para el workflow:

| Clave                  | Valor                                      |
|------------------------|--------------------------------------------|
| S3_BUCKET              | data-engineer-juanmgart                    |
| PATH_RAW_DATA          | rawData/housing_data/raw_data.parquet      |
| TABLE_NAME             | final_real_estate_insumo                   |
| DATABASE_NAME          | trusted_data                               |
| OUTPUT_TABLE_PATH_S3   | output_files              |
| GLUE_CATALOG           | AwsDataCatalog                             |

## Estructura del Proyecto
El script principal del proyecto es `DataWkflow-Job.py`, que incluye las siguientes funciones:
- `read_parquet_s3(bucket: str, key: str) -> DataFrame`: Lee un archivo Parquet desde S3.
- `data_cleaning(df: DataFrame) -> DataFrame`: Limpia los datos eliminando valores nulos y duplicados.
- `calculate_final_df(df: DataFrame) -> DataFrame`: Calcula columnas adicionales para el conjunto de datos final.
- `save_to_s3(df: DataFrame, bucket: str, key: str) -> None`: Guarda el conjunto de datos final en S3.

## ğŸŒŸ Flujo General de Trabajo
El flujo de trabajo incluye las siguientes ramas y ambientes:

| **Ambiente**  | **Rama**     | **PropÃ³sito**                                              |
|---------------|--------------|------------------------------------------------------------|
| ğŸŒ± **Develop**  | `develop`    | Desarrollo y pruebas iniciales.                           |
| ğŸš€ **Release**  | `release`    | ValidaciÃ³n previa al despliegue en producciÃ³n.            |
| ğŸ† **ProducciÃ³n**| `main`       | CÃ³digo estable y aprobado en producciÃ³n.                  |

## ğŸ“‚ OrganizaciÃ³n del Repositorio
ğŸ“¦ data-workflow-project
â”œâ”€â”€ ğŸ“‚ aws-Resrcs
â”‚   â”œâ”€â”€ ğŸ“‚ azure-pipelines.yml
â”‚   â”œâ”€â”€ ğŸ“‚ infraCloudformation-params.json
â”‚   â”œâ”€â”€ ğŸ“‚ infraCloudformation.yaml
â”œâ”€â”€ ğŸ“‚ glue-job
â”‚   â”œâ”€â”€ ğŸ“‚ DataWkflow-Job.py
â””â”€â”€ README.md

## ExplicaciÃ³n de la OrganizaciÃ³n del Proyecto
He organizado el proyecto de esta manera para mantener una estructura clara y modular que facilite el desarrollo y mantenimiento del mismo:

- **aws-Resrcs**: Esta carpeta contiene los archivos relacionados con la infraestructura en la nube, como los parÃ¡metros y plantillas de CloudFormation, y el archivo de configuraciÃ³n de Azure Pipelines. Esto permite gestionar y desplegar la infraestructura de manera automatizada y reproducible.
- **glue-job**: Esta carpeta contiene el script principal del trabajo de AWS Glue (`DataWkflow-Job.py`). Al separar el cÃ³digo del trabajo de Glue en su propia carpeta, se facilita la gestiÃ³n y el desarrollo del cÃ³digo ETL.
- **README.md**: El archivo README proporciona una descripciÃ³n general del proyecto, incluyendo el objetivo, las preguntas a abordar, la elecciÃ³n del modelo, los enfoques para diferentes escenarios, los parÃ¡metros del workflow y la estructura del proyecto. TambiÃ©n incluye una imagen de la arquitectura y la fuente de los datos.

Esta organizaciÃ³n permite una clara separaciÃ³n de responsabilidades y facilita la colaboraciÃ³n y el mantenimiento del proyecto.

## Paso a paso creaciÃ³n proyecto en AWS

1. **Crear el Stack en CloudFormation**
   - Sube el archivo `infraCloudformation.yaml` a AWS CloudFormation para crear el stack.
   - Esto configurarÃ¡ la infraestructura necesaria para el proyecto, incluyendo roles IAM, buckets S3 y otros recursos necesarios.
   ![CloudFormation Stack](sources/stack.png)

2. **AÃ±adir el Dataset al Data Lake (S3)**
   - Sube el dataset al Data Lake en Amazon S3 en formato Parquet para un mejor procesamiento.
   - El formato Parquet es eficiente en tÃ©rminos de almacenamiento y rendimiento de consulta.
   ![Subir Dataset a S3](sources/parquet_file.png)

4. **Crear y Configurar el Job en AWS Glue**
   - En la consola de AWS Glue, navega a la secciÃ³n de Jobs y crea un nuevo Job.
   - Asigna un nombre al Job, por ejemplo, `PRJ001-Glue-Project-GlueJob-ETL-Data-Workflow-DEV`.
   - Selecciona el rol IAM creado por el stack de CloudFormation.
   - Configura el script del Job para que apunte al archivo `DataWkflow-Job.py` en tu bucket S3.
   - Configura las opciones de ejecuciÃ³n, como el tipo de instancia y el nÃºmero de workers.
   ![Crear Job en AWS Glue](sources/job.png)

5. **Programar el Job en AWS Glue**
   - Crea un trigger en AWS Glue para programar la ejecuciÃ³n del Job.
   - Configura el trigger para que se ejecute diariamente a las 12:00 PM UTC.
   - Asocia el trigger con el Job `PRJ001-Glue-Project-GlueJob-ETL-Data-Workflow-DEV`.
   ![Programar Job en AWS Glue](sources/trigger.png)

6. **Ejecutar y Monitorear el Job**
   - Ejecuta el Job manualmente la primera vez para asegurarte de que todo estÃ© configurado correctamente.
   - Monitorea la ejecuciÃ³n del Job en la consola de AWS Glue para verificar que se complete sin errores.
   - Revisa los logs generados en CloudWatch para obtener detalles sobre la ejecuciÃ³n del Job.
   ![Monitorear Job en AWS Glue](sources/logs.png)
   ![Monitorear Job en AWS Glue](sources/runs.png)